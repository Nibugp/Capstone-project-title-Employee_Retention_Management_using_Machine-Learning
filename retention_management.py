# -*- coding: utf-8 -*-
"""Retention_Management

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qACD_iui4lxWnjsXa8BtV2401d_hCKDx

1. **Import the neccessary libraries**
"""

!pip install imbalanced-learn

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')
from sklearn.preprocessing import LabelEncoder, StandardScaler
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
import numpy as np
from keras.models import Sequential
from keras.layers import Dense, Dropout, BatchNormalization
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping, LearningRateScheduler
from sklearn.metrics import classification_report,confusion_matrix, roc_curve, roc_auc_score
import scipy.stats as stats
import shap  # Import SHAP

"""**Load the data**"""

# Load your data
df = pd.read_excel('/content/Employee_data_.xlsx')

"""**Data Overview**"""

print("\nFirst 5 Rows:")
print(df.head())
print("\nDataset Columns:")
print(df.columns)
print("Dataset Overview:")
print(df.info())

# Setting display options to avoid scientific notation
pd.set_option('display.float_format', '{:.2f}'.format)

# Descriptive statistics
print("\nDescriptive Statistics:")
print(df.describe())

# Checking for missing values
print("\nMissing Values:")
print(df.isnull().sum())

"""**Exploratory Data Analysis**"""

## Univariate Analysis: Numeric Columns

def plot_numeric_distributions(df):
    numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns
    if 'EEID' in numeric_columns:
        numeric_columns = numeric_columns.drop('EEID')  # Exclude EEID if present

    if len(numeric_columns) == 0:
        print("No numeric columns to plot.")
        return

    n_cols = 2  # Number of plots per row
    n_rows = (len(numeric_columns) + n_cols - 1) // n_cols  # Calculate required rows
    plt.figure(figsize=(10, n_rows * 4))  # Adjust figure size

    for i, col in enumerate(numeric_columns, 1):
        plt.subplot(n_rows, n_cols, i)
        sns.histplot(df[col].dropna(), bins=30, color='skyblue', kde=True)

        # Customizing the KDE line using kdeplot directly
        sns.kdeplot(df[col].dropna(), color='darkblue', linewidth=2)

        plt.title(f'Distribution of {col}')
        plt.xlabel(col)
        plt.ylabel('Frequency')

    plt.tight_layout()
    plt.show()

# Call the function to plot distributions
plot_numeric_distributions(df)

## Univariate Analysis: Categorical Columns

def plot_categorical_counts(df):
    # Explicitly exclude the 'Name' column from being plotted
    categorical_columns = df.select_dtypes(include=['object', 'category']).columns

    # Remove 'Name' if it exists
    categorical_columns = [col for col in categorical_columns if col.lower() != 'name']

    plt.figure(figsize=(10, len(categorical_columns) * 3))

    for i, col in enumerate(categorical_columns, 1):
        plt.subplot(len(categorical_columns), 1, i)
        sns.countplot(y=df[col])
        plt.title(f'Count of {col}')
        plt.tight_layout()

    plt.show()

# Run the function
plot_categorical_counts(df)

##Bivariate Analysis

# Bivariate Analysis: Delivery Unit vs Attrition
plt.figure(figsize=(10, 6))
sns.countplot(x='Delivery Unit', hue='Attrition', data=df)
plt.title('Attrition Across Delivery Units')
plt.xticks(rotation=45)
plt.show()

# Bivariate Analysis: Education_classification vs attrition
plt.figure(figsize=(10, 6))
sns.countplot(x='Education', hue='Attrition', data=df)
plt.title('Attrition Across Education')
plt.xticks(rotation=45)
plt.show()

# Bivariate Analysis: Gender vs Attrition
plt.figure(figsize=(10, 6))
sns.countplot(x='Gender', hue='Attrition', data=df)
plt.title('Attrition Across Gender')
plt.xticks(rotation=45)
plt.show()

# Bivariate Analysis: location vs attrition
plt.figure(figsize=(10, 6))
sns.countplot(x='Location', hue='Attrition', data=df)
plt.title('Attrition Across location')
plt.xticks(rotation=45)
plt.show()

# Bivariate Analysis: Grade vs attrition
plt.figure(figsize=(10, 6))
sns.countplot(x='Grade', hue='Attrition', data=df)
plt.title('Attrition Across Grade')
plt.xticks(rotation=45)
plt.show()

# Bivariate Analysis: Average rating vs Attrition
plt.figure(figsize=(10, 6))
sns.countplot(x='Average rating', hue='Attrition', data=df)
plt.title('Attrition Across Average rating')
plt.xticks(rotation=45)
plt.show()

"""**Data Cleaning & Preprocessing**"""

## Dropping Unnecessary Columns
# Drop 'EEID' column
df = df.drop(columns=['EEID'], errors='ignore')
print(f"Data shape after removing EEID: {df.shape}")

## Handling Missing Values
# Fill missing ratings with 0
rating_columns = ['FY 21 Rating', 'FY 22 rating', 'FY 23 Rating', 'Average rating']
df[rating_columns] = df[rating_columns].fillna(0)

## Categorizing and Encoding Education Levels
# Define categorization function for education levels
def categorize_education(education):
    undergraduate = ['BE', 'B.Tech', 'B. Sc. (Engineering)', 'B. Com.', 'BCA', 'Diploma']
    postgraduate = [
        'M.Tech.', 'ME', 'MBA', 'MS', 'M.Sc. (Engineering)', 'M. Tech',
        'MCA', 'M. Com.', 'M.Sc. (Technology)', 'MA', 'AMIE'
    ]
    if education in undergraduate:
        return 'Undergraduate'
    elif education in postgraduate:
        return 'Postgraduate'
    else:
        return 'Others'

# Apply categorization and encode 'Education'
df['Education_Classification'] = df['Education'].apply(categorize_education)
education_mapping = {'Undergraduate': 1, 'Postgraduate': 2, 'Others': 3}
df['Education_Classification_Numeric'] = df['Education_Classification'].map(education_mapping)

## Dropping Redundant Columns

df = df.drop(columns=['Category', 'Degree_Type', 'Education_Classification'], errors='ignore')

## Encoding Categorical Columns

label_encoder = LabelEncoder()
df['Attrition'] = label_encoder.fit_transform(df['Attrition'])
df['Gender'] = label_encoder.fit_transform(df['Gender'])

## Handling Dates and Calculating Promotion-related Features

# Convert promotion date columns to datetime
df['DOLP 1'] = pd.to_datetime(df['DOLP 1'], errors='coerce')
df['DOLP 2'] = pd.to_datetime(df['DOLP 2'], errors='coerce')

# Create indicators for missing promotion dates
df['DOLP1_missing'] = df['DOLP 1'].isnull().astype(int)
df['DOLP2_missing'] = df['DOLP 2'].isnull().astype(int)

# Calculate promotion-related features
df['Promotion Frequency'] = df[['DOLP 1', 'DOLP 2']].notna().sum(axis=1)
df['Last_Promotion_Date'] = df['DOLP 2'].fillna(df['DOLP 1'])
df['Months Since Last Promotion'] = (pd.to_datetime('today') - df['Last_Promotion_Date']).dt.days // 30
df['Time Between Promotions'] = (df['DOLP 2'] - df['DOLP 1']).dt.days // 30
df['Time Between Promotions'] = df['Time Between Promotions'].fillna(0).astype(int)

# Fill NaN values in 'Months Since Last Promotion' column with 0
df['Months Since Last Promotion'] = df['Months Since Last Promotion'].fillna(0)

# Fill missing dates with a placeholder (if still NaN after calculations)
df[['DOLP 1', 'DOLP 2', 'Last_Promotion_Date']] = df[['DOLP 1', 'DOLP 2', 'Last_Promotion_Date']].fillna(pd.Timestamp(0))

# Calculate tenure in months
df['Date of joining'] = pd.to_datetime(df['Date of joining'], errors='coerce')
current_date = pd.to_datetime("today")
df['Tenure_in_months'] = (current_date - df['Date of joining']).dt.days // 30

# Perform one-hot encoding on 'Delivery Unit' and 'Location'
df = pd.get_dummies(df, columns=['Delivery Unit', 'Location'], drop_first=True)

# Map and encode 'Grade' (Ordinal encoding)
grade_mapping = {f'TS-{i}': i for i in range(1, 13)}  # Map TS-1 to TS-12
df['Grade'] = df['Grade'].map(grade_mapping)

## Dropping Unused Columns
unused_columns = ['Name', 'Date of joining','Cutoof date','Education', 'DOLP 1', 'DOLP 2', 'Last_Promotion_Date',
                  'Education','Education_Classification', 'DOLP1_encode', 'DOLP2_encode']
df = df.drop(columns=unused_columns, errors='ignore')

"""**Correlation Analysis**"""

# Reorder columns to move 'Attrition' to the end
cols = [col for col in df.columns if col != 'Attrition'] + ['Attrition']
df_reordered = df[cols]

# Select only numeric columns
numeric_df = df_reordered.select_dtypes(include=['float64', 'int64'])
if 'EEID' in numeric_df.columns:
    numeric_df = numeric_df.drop(columns=['EEID'])

# Calculate the Pearson correlation matrix
correlation_matrix = numeric_df.corr()

# Plotting the heatmap
plt.figure(figsize=(21, 18))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="coolwarm", annot_kws={'size': 15})
plt.title("Correlation Heatmap")
plt.show()

"""**Feature Scaling and Data Balancing**"""

## Feature Scaling

scaler = StandardScaler()
scaled_features = scaler.fit_transform(df.drop(columns=['Attrition']))
df_scaled = pd.DataFrame(scaled_features, columns=df.drop(columns=['Attrition']).columns)

# Add target column back to scaled dataframe
df_scaled['Attrition'] = df['Attrition']

## Handling Imbalanced Data
# Address imbalanced data using SMOTE
X = df_scaled.drop(columns=['Attrition'])
y = df_scaled['Attrition']
smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(X, y)

"""**Splitting Data**"""

## Splitting Data
# Split into train and validation sets
X_train, X_val, y_train, y_val = train_test_split(X_res, y_res, test_size=0.2, random_state=42)

"""**Model Building and Compilation**"""

## Model Architecture

# Define the model architecture
model = Sequential([
    Dense(64, input_dim=X_train.shape[1], activation='relu'),
    BatchNormalization(),
    Dropout(0.5),
    Dense(64, activation='relu'),
    BatchNormalization(),
    Dropout(0.5),
    Dense(32, activation='relu'),
    BatchNormalization(),
    Dropout(0.5),
    Dense(1, activation='sigmoid')  # Use sigmoid for binary classification
])

## Model Compilation

# Learning rate optimizer with a lower rate
optimizer = Adam(learning_rate=0.0005)

# Compile the model
model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])

"""**Model Training**"""

## Model Training

# Early stopping callback to avoid overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Learning rate scheduler (optional, for finer control)
def lr_schedule(epoch):
    if epoch < 20:
        return 0.001
    else:
        return 0.0005

lr_scheduler = LearningRateScheduler(lr_schedule)

# Fit the model with early stopping and learning rate scheduler
history = model.fit(X_train, y_train, epochs=150, batch_size=32, validation_data=(X_val, y_val),
                    callbacks=[early_stopping, lr_scheduler])

"""Model Evaluation"""

# Evaluate the model on the validation data
val_loss, val_acc = model.evaluate(X_val, y_val)
print(f'Validation Loss: {val_loss}')
print(f'Validation Accuracy: {val_acc}')

"""Making Predictions"""

## Making Predictions
# Predict on the validation set
y_pred = model.predict(X_val)

# If you want to get a classification report
print(classification_report(y_val, y_pred.round()))

y_pred_prob = model.predict(X_val)  # Ensure this step runs before the confusion matrix
y_pred = (y_pred_prob > 0.5).astype(int)  # Convert probabilities to binary (0 or 1)

## Confusion Matrix
cm = confusion_matrix(y_val, y_pred)
plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# ROC Curve and AUC
fpr, tpr, thresholds = roc_curve(y_val, y_pred_prob)
roc_auc = roc_auc_score(y_val, y_pred_prob)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

"""SHAP Analysis"""

# Create SHAP Explainer using the trained model and training data
explainer = shap.Explainer(model, X_train)

# Calculate SHAP values for the validation data
shap_values = explainer(X_val)

# Visualize SHAP summary plot
shap.summary_plot(shap_values, X_val)

"""**Analysis and Results**"""

# Example Test Data - New Employee Profile
test_data = pd.DataFrame({
    'FY 21 Rating': [4.2],
    'FY 22 rating': [4.5],
    'FY 23 Rating': [4.3],
    'Average rating': [4.33],
    'Education_Classification_Numeric': [2],  # Postgraduate
    'Gender': [1],  # Female
    'DOLP1_missing': [0],
    'DOLP2_missing': [1],
    'Months Since Last Promotion': [42],  # Last promotion was 42 months ago
    'Time Between Promotions': [36],  # Previous promotion cycle was 36 months
    'Tenure_in_months': [72],  # Employee has worked for 6 years (72 months)
    'Grade': [7],  # Mid-level grade
    'Delivery Unit_Unit A': [1],
    'Delivery Unit_Unit B': [0],
    'Location_Location X': [1],
    'Location_Location Y': [0]
})

# Ensure test data has same features as training set
test_data = test_data.reindex(columns=X.columns, fill_value=0)

# Scale test data using trained scaler
test_data_scaled = scaler.transform(test_data)

# Make Prediction
test_prediction = model.predict(test_data_scaled).round()

# Interpretation
prediction_label = "Leave" if test_prediction[0][0] == 1 else "Stay"
print(f"Predicted Attrition Status: {prediction_label}")

# Probability of leaving
probability_of_leaving = model.predict(test_data_scaled)[0][0] * 100
print(f"Probability of Leaving: {probability_of_leaving:.2f}%")